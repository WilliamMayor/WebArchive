---
layout: post
title: Technical Approaches to Collection
tags: [review, collection]
---

This post is a short summary of the current approaches to the collection phase of web archiving. 

## Remote Crawl

Remotely crawled archives consist of web pages gathered by web crawlers (sometimes called robots). They are remote in the sense that they operate on machines separate from those that host the webpage being archived. They crawl in the sense that they parse each downloaded page looking for new links to new pages. The crawler is provided with a list of seed URLs it visits the URLs in turn and downloads the HTML found there. It then parses the HTML, looking for links to new content. Links could take the form of HTML anchors (`<a>` tags with `href` attributes), iframes, dynamically generated URLs (e.g. using JavaScript), and many more. The crawler adds the discovered URLs to the list and repeats the process. The URL list can be filtered by a selection process, as can the downloaded HTML. In many websites the downloaded content contains more than just HTML (e.g. streamed video or a Flash application). Crawlers have to be written to take in account these, more complicated, content types.

Centralised remote crawling (e.g. Heritrix) is where a single machine (often a single application) performs a web crawl. This is advantageous in that there is no wasted effort, the single machine knows which URLs to crawl and will not duplicate any work. It is also much simpler to build, maintain, organise, and run a centralised crawler as there are far fewer parts and little concurrency. Unfortunately, they are often slow as they are limited as to how many URLs they can crawl simultaneously. 

Distributed crawlers provide a good speed up in crawling rate. Theoretically, there could be a linear speed up (adding a new computer always provides the same speed boost), in practise the collaboration between the machines is likely to inhibit this rate. The downsides to a distributed crawler comes in the increased complexity of the system. Computers in a distributed network have to communicate with each other to ensure that new URLs are being crawled and repetition is not too high. This collaboration can be roughly categorised into one of two forms. The first involves using a master computer that is responsible for sharing out the workload. When a crawler finds a new URL it sends it to the master server that will decide which crawler to forward it on to. This system has great load balancing capabilities and can provide a steady throughput. The second form involves each crawler becoming responsible for an area of the archive. Commonly, a crawler would take a particular domain, but the division could be according to for instance topic, or written language. When a crawler finds a new URL it tries to find the crawler that would be responsible for this URL and sends it on. This system does not require a controlling server and so each crawler can operate largely independently. This is great when the crawlers are distributed across a wide geographical area where latency can become a problem. Unfortunately, this division of labour can lead to unbalanced workloads. One domain might contain many more pages than another leading one machine to be constantly crawling and another to be relatively quiet.

## Transactional

Remote crawling techniques focus on collecting the content of a webpage at the time of access. There is much contextual data lost with such a process, some of which can be kept using transactional archiving. A transactional archive stores not just the content of pages but the transactions between web servers and web clients (browsers). In doing so it can build a picture of a visitor's route through a web page, including, for instance, pages visited in which order, and length of stay on each page. Transactional archiving can also have the benefit of storing the differences seen by different users visiting the same page. Differences arise when different ads are served or if the page is personalised to a particular user. There are issues of privacy that a transactional archive must address. Not only with storing personalised (and potentially personal) versions of web pages but also of issues with series of transactions being used to identify otherwise anonymous users.

The most common transactional archives are server-side systems. A webmaster installs some software into their web server that records the pages served up to the users. This system provides a robust view of that site's traffic and content. Every visitor will be recorded and all transactions will be found. Unfortunately, it requires total cooperation of the webmaster, something that may be unlikely in many cases. The cost burden (in terms of monetary expense and computational) falls on the webmaster but potentially very few benefits are seen.

Alternate to server-side transactional archives is the possibility of client-side collection. This has not been explored much currently, but has the potential to provide some interesting benefits. The cost of the archive to each user is significantly lower than the cost to a webmaster. A single user's transactions are collectively very manageable, it is the sum total over every user that introduces the expense. Also, client-side transactions will provide routing information that is much less restricted in scope. If a web page links a user to an external page, the client can record these transactions where the server would have lost them. This could lead to information about a user's global use of the Internet.

## Direct

Direct web archiving is another process that involves the cooperation of the webmaster to work. It involves simply making a copy of the entire website, as found on the web server. This leads to a very complete picture of the web site as no pages can be missing. It also offers the possibility of archiving more than just the front-facing HTML code. A direct archive could also involve making a copy of a site's database and server-side code. This has the benefit of allowing the archive to maintain complex functionality, like search, or AJAX requests. Detailed archives such as this introduce complications in the way they are to be stored and preserved. If a web site can only work with particular software then care must be taken to archive the entire ecosystem, or to convert the ecosystem to a standard one. It is possible to create an archive of emulated sites, where each site is accompanied by the software required to run it. This process is expensive and complex, emulators may become too out-dated to run, and if important parts of the archive are left out (or go missing) then the entire site becomes unreadable. Alternatively, the site can be migrated to a common format, this has the benefit of becoming easier to store and read, there is a greater likelihood of continued access into the future. Migration may lose some desirable contextual data that emulation would keep.